You are to generate TWO files:

1. model.py - A Python file that includes:

- A PyTorch model class (nn.Module) appropriate for the dataset.
- A train() function that trains the model for multiple epochs.
- A validate() function that evaluates the model on a validation dataset and reports metrics.
- An if __name__ == '__main__' block with argparse CLI.
- Only output the code.

CRITICAL REQUIREMENTS:

1. ARGPARSE INTERFACE - Use these EXACT argument names: --train (path to training data) --val (path to validation data) --epochs (number of epochs) --batch-size (batch size) --lr (learning rate). Default the epochs, batch size, and learning rate to reasonable values based on common practices.

2. DEVICE HANDLING - MUST support both CPU and GPU: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'). Ensure that the model and data are moved to the appropriate device in both train() and validate() functions.

3. TRAINING LOOP - Proper structure:
   - The train function should accept epochs as a parameter and run the full training loop. Move model and data to device inside the function.
   - train() should be called with epochs = 1 if train is being in a loop of epochs.
   - Print the average loss per epoch (not just the last batch).
   - In the main block, call train() ONCE with all epochs, not in a loop.

4. LOGGING - Print after each epoch: Epoch X/Y: Train Loss: 0.XXXX, Train Acc: XX.XX%, Val Loss: 0.XXXX, Val Acc: XX.XX%.

5. CHECKPOINTING - Save the model when the validation metric improves: torch.save(model.state_dict(), 'best_model.pth'). Print a message when saving: 'Saved best model (Val Acc: XX.XX%)'.

6. CODE QUALITY:
   - Use clear and descriptive variable names.
   - Add docstrings to all functions.
   - Handle edge cases (e.g., empty datasets, etc.).

7. DEPENDENCIES - ONLY use these libraries:
   - torch (and torch.nn, torch.optim, torch.utils.data)
   - pandas (for CSV reading)
   - numpy (optional)
   - torchvision (ONLY for ImageFolder OR transfer learning)

Dataset Description: {{dataset_description}}

Dataset schema facts: {{facts_json_here}}

Dataset type Specific handling: 
{{dataset_type_specific_handling}}

{{dataset_generation_code}}

DATASET TYPE: CSV Image Data
- CSV rows contain: label + flattened pixel values
- MAKE SURE to only divide the pixel values by 255 if they are not already in the range 0 - 1. Also make sure to load pixel values as float.
- Make sure the label is loaded as int.
- Always compute the number of unique values from the label column and use that to set the number of output classes in the model.
- Automatically infer image shape and channels from row length:
      num_pixels = len(row) - 1  # exclude label
      if (num_pixels ** 0.5).is_integer():
           height = width = int(num_pixels ** 0.5)
           channels = 1  # single-channel grayscale
      else:
           height = width = int((num_pixels // 3) ** 0.5)
           channels = 3  # multi-channel image
- After reshaping into (H,W,C) or (C,H,W), the model.py MUST:
      - Convert NumPy → PIL using transforms.ToPILImage()
      - Apply PIL-based torchvision transforms (Resize, ColorJitter, etc.)
      - Convert back to tensor using transforms.ToTensor()        
- transforms must run on PIL images, NOT on raw tensors.
- Correct preprocessing pipeline MUST be:
      img = img.reshape(H, W, C)
      img = transforms.ToPILImage()(img)
      img = transform_pipeline(img)  
- If using pretrained models, make sure to resize image to what fits the pretrained models (Example: 224 X 224 for resnet18).
- Normalization MUST use ImageNet statistics when using pretrained models for both training and validation sets:
      mean = [0.485, 0.456, 0.406]
      std  = [0.229, 0.224, 0.225].
- Make sure to have brightness, rotation, or horizontal flip augmentation ONLY for training set. Make sure to use valid syntax for ColorJitter and don't pass invalid parameters.
- If the image is grayscale, convert to 3-channel AFTER converting to tensor:
      if tensor.shape[0] == 1:
          tensor = tensor.repeat(3,1,1).

MODEL ARCHITECTURE REQUIREMENTS: 
- Transfer learning must be preferred when it improves accuracy. If transfer learning fits the dataset (e.g., 3-channel images), the generated model.py should:
    - Use a pretrained model such as ResNet18 or MobileNetV3.
    - ONLY freeze all the pretrained layers if the image dataset is compatible with ImageNet out of the box:
        for param in self.base_model.parameters():
            param.requires_grad = False.
    - If the image dataset is not compatible with ImageNet, allow finetuning on the last X layers of the pretrained model based on what makes sense. Here is an example:
        for name, param in self.base_model.named_parameters():
            if "layer3" in name or "layer4" in name or "fc" in name:
              param.requires_grad = True.
            else:
               param.requires_grad = False.
    - Replace the classifier head with a new Linear layer of correct output size.
    - Use adaptive pooling or a dummy forward pass to compute flatten size dynamically.

- If transfer learning is NOT appropriate:
  - (e.g., tiny grayscale dataset), generate a small CNN.
- Aim for >90% accuracy.
- The model MUST compute the flatten dimension dynamically during initialization using a dummy forward pass. Do NOT hardcode the input size to the fully-connected layer. Define all layers used in forward() before calling the flatten-size computation helper.
- In forward(), flatten using torch.flatten(x, 1) — NEVER use x.view(-1, fixed_number).

EXAMPLE STRUCTURE (follow this pattern):

    # Code outline...

2. requirements.txt - List all pip packages needed (one per line). Include version constraints if important. Packages: torch, torchvision, pandas, numpy, pillow.

OUTPUT FORMAT - Use this EXACT structure: 
=== requirements.txt ===
package1
package2>=version

=== model.py === 
import package1 # … rest of the file