You are to generate TWO files:

1. model.py - A Python file that includes:
- A train() function that trains the model for multiple epochs.
- A validate() function that evaluates the model on a validation dataset and reports metrics.
- An if __name__ == '__main__' block with argparse CLI.
- Only output the code.

CRITICAL REQUIREMENTS:

1. ARGPARSE INTERFACE - Use these EXACT argument names: --train (path to training data) --val (path to validation data) --epochs (number of epochs) --batch-size (batch size) --lr (learning rate). Default the epochs, batch size, and learning rate to reasonable values based on common practices.

2. DEVICE HANDLING - MUST support both CPU and GPU: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'). Ensure that the model and data are moved to the appropriate device in both train() and validate() functions.

3. TRAINING LOOP - Proper structure:
   - The train function should accept epochs as a parameter and run the full training loop. Move model and data to device inside the function.
   - train() should be called with epochs = 1 if train is being in a loop of epochs.
   - Print the average loss per epoch (not just the last batch).
   - In the main block, call train() ONCE with all epochs, not in a loop.

4. LOGGING - Print after each epoch: Epoch X/Y: Train Loss: 0.XXXX, Train Acc: XX.XX%, Val Loss: 0.XXXX, Val Acc: XX.XX%.

5. CHECKPOINTING - Save the model when the validation metric improves: torch.save(model.state_dict(), 'best_model.pth'). Print a message when saving: 'Saved best model (Val Acc: XX.XX%)'.

6. CODE QUALITY:
   - Use clear and descriptive variable names.
   - Add docstrings to all functions.
   - Handle edge cases (e.g., empty datasets, etc.).

7. DEPENDENCIES - ONLY use these libraries:
   - torch (and torch.nn, torch.optim, torch.utils.data)
   - pandas (for CSV reading)
   - numpy (optional)
   - torchvision (ONLY for ImageFolder OR transfer learning)

8. MODEL CLASS - Use a pretrained model such as ResNet18 or MobileNetV3 UNLESS IT IS A VERY SIMPLE DATASET.
    - ONLY freeze all the pretrained layers if the image dataset is compatible with ImageNet out of the box:
        for param in self.base_model.parameters():
            param.requires_grad = False.
    - If the image dataset is not compatible with ImageNet, allow finetuning on the last X layers of the pretrained model based on what makes sense. Here is an example:
        for name, param in self.base_model.named_parameters():
            if "layer3" in name or "layer4" in name or "fc" in name:
              param.requires_grad = True.
            else:
               param.requires_grad = False.
    - Replace the classifier head with a new Linear layer of correct output size.
    - Use adaptive pooling or a dummy forward pass to compute flatten size dynamically.
- Aim for >90% accuracy.
- The model MUST compute the flatten dimension dynamically during initialization using a dummy forward pass. Do NOT hardcode the input size to the fully-connected layer. Define all layers used in forward() before calling the flatten-size computation helper.
- In forward(), flatten using torch.flatten(x, 1) — NEVER use x.view(-1, fixed_number).

Dataset Description: {{dataset_description}}

Dataset schema facts: {{facts_json_here}}

Dataset type Specific handling: 
{{dataset_type_specific_handling}}

{{dataset_generation_code}}

If the dataset type is huggingface:
- Use huggingface datasets to load data: 
      from datasets import load_dataset
      train_hf = load_dataset(<Use whatever is passed as --train argument and remove train at end of path if it exists>, split="train")
      test_hf = load_dataset(<Use whatever is passed as --test argument and remove test at end of path if it exists>, split="test")
- The dataset contains:
 - image feature: either a PIL image or a NumPy array or list representing pixel values. 
       - If image is list, convert it to numpy array with dtype uint8.
       - image preprocessing must be able to handle both PIL image and numpy array image input
 - label feature: ClassLabel or int64 Value 
- Labels must be loaded as int and num_classes must be computed from the label feature to set the model output size:
     - label_feature = train_hf.features["label"]
       # --- Robust num_classes detection ---
       if hasattr(label_feature, "num_classes"):
           # Case 1: ClassLabel
           num_classes = label_feature.num_classes
       else:
           # Case 2: Value(int) -> infer unique labels
           # Efficient: take unique from arrow without loading all
           unique_vals = ds.unique("label")
           num_classes = len(unique_vals)
- If image is a NumPy array:
     -Reshape to (H, W, C) where: 
        - If num_pixels = image.size is a perfect square → grayscale, C=1
        - Otherwise → C=3 for color images
     -Convert to PIL: img = transforms.ToPILImage()(image)
- The preprocess function used with .map() MUST return a dict with keys 'image' and 'label', NEVER a tuple.
- DataLoaders must use collate_fn to convert dicts to (images, labels) batches.
- Apply torchvision transforms on PIL images, then convert to tensor.
- Convert single-channel grayscale tensors to 3 channels when using pretrained models.

If the dataset type is CSV Image Data:
- CSV rows contain: label + flattened pixel values
- MAKE SURE to only divide the pixel values by 255 if they are not already in the range 0 - 1. Also make sure to load pixel values as float.
- Make sure the label is loaded as int.
- Always compute the number of unique values from the label column and use that to set the number of output classes in the model.
- Automatically infer image shape and channels from row length:
      num_pixels = len(row) - 1  # exclude label
      if (num_pixels ** 0.5).is_integer():
           height = width = int(num_pixels ** 0.5)
           channels = 1  # single-channel grayscale
      else:
           height = width = int((num_pixels // 3) ** 0.5)
           channels = 3  # multi-channel image

FOR BOTH CSV AND HUGGINGFACE DATASET:
- After reshaping into (H,W,C) or (C,H,W), the model.py MUST:
      - Convert NumPy → PIL using transforms.ToPILImage()
      - Apply PIL-based torchvision transforms (Resize, ColorJitter, etc.)
      - Convert back to tensor using transforms.ToTensor()        
- transforms must run on PIL images, NOT on raw tensors.
- Correct preprocessing pipeline MUST be:
      img = img.reshape(H, W, C)
      img = transforms.ToPILImage()(img)
      img = transform_pipeline(img)  
- If using pretrained models, make sure to resize image to what fits the pretrained models (Example: 224 X 224 for resnet18).
- Normalization MUST use ImageNet statistics when using pretrained models for both training and validation sets:
      mean = [0.485, 0.456, 0.406]
      std  = [0.229, 0.224, 0.225].
- Make sure to have brightness, rotation, or horizontal flip augmentation ONLY for training set. Make sure to use valid syntax for ColorJitter and don't pass invalid parameters.
- If the image is grayscale, convert to 3-channel AFTER converting to tensor:
      if tensor.shape[0] == 1:
          tensor = tensor.repeat(3,1,1).

EXAMPLE STRUCTURE (follow this pattern):

    # Code outline...

2. requirements.txt - List all pip packages needed (one per line). Include version constraints if important. Packages: torch, torchvision, pandas, numpy, pillow.

OUTPUT FORMAT - Use this EXACT structure: 
=== requirements.txt ===
package1
package2>=version

=== model.py === 
import package1 # … rest of the file