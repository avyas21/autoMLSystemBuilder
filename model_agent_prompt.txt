You are to generate TWO files:

1.  model.py - A Python file that includes:

-   A PyTorch model class (nn.Module) appropriate for the dataset.
-   A train() function that trains the model for multiple epochs.
-   A validate() function that evaluates the model on a validation dataset and reports metrics.
-   An if __name__ == '__main__' block with argparse CLI.

CRITICAL REQUIREMENTS:

1.  ARGPARSE INTERFACE - Use these EXACT argument names: --train (path to training data) --val (path to validation data) --epochs (number of epochs) --batch-size (batch size) --lr (learning rate). Default the epochs, batch size, and learning rate to reasonable values based on common practices.

2.  DEVICE HANDLING - MUST support both CPU and GPU: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'). Ensure that the model and data are moved to the appropriate device in both train() and validate() functions.

3.  TRAINING LOOP - Proper structure:
    -   The train function should accept epochs as a parameter and run the full training loop.
    -   Print the average loss per epoch (not just the last batch).
    -   In the main block, call train() ONCE with all epochs, not in a loop.

4.  LOGGING - Print after each epoch: Epoch X/Y: Train Loss: 0.XXXX, Train Acc: XX.XX%, Val Loss: 0.XXXX, Val Acc: XX.XX%.

5.  CHECKPOINTING - Save the model when the validation metric improves: torch.save(model.state_dict(), 'best_model.pth'). Print a message when saving: 'Saved best model (Val Acc: XX.XX%)'.

6.  CODE QUALITY:
    -   Use clear and descriptive variable names.
    -   Add docstrings to all functions.
    -   Handle edge cases (e.g., empty datasets, etc.).

7.  DEPENDENCIES - ONLY use these libraries:
    -   torch (and torch.nn, torch.optim, torch.utils.data)
    -   pandas (for CSV reading)
    -   numpy (optional)
    -   torchvision (ONLY for ImageFolder OR transfer learning)

Dataset Description: {{dataset_description}}

Dataset schema facts: {{facts_json_here}}

Dataset type Specific handling: 
{{dataset_type_specific_handling}}

DATASET TYPE: CSV Image Data
 - CSV rows contain: label + flattened pixel values
 - Automatically infer image shape and channels from row length:
        num_pixels = len(row) - 1  # exclude label
        if (num_pixels ** 0.5).is_integer():
             height = width = int(num_pixels ** 0.5)
             channels = 1  # single-channel grayscale
        else:
             height = width = int((num_pixels // 3) ** 0.5)
             channels = 3  # multi-channel image
        - Reshape tensor:
          img = img.reshape(channels, height, width)
          Repeat channels to 3 if single-channel:
          if channels == 1:
            img = img.repeat(3, 1, 1)
        - Do NOT use transforms.ToPILImage()
        - Use tensor transforms: Resize(...), Normalize([...]) when needed
        - If the dataset contains grayscale images that must be expanded to 3 channels, the model.py MUST first convert the NumPy image array into a torch tensor BEFORE calling .repeat() 

MODEL ARCHITECTURE REQUIREMENTS: 
- Aim for >90% accuracy.
- Use the simplest architecture that would yield good results.
- The model MUST compute the flatten dimension dynamically during initialization using a dummy forward pass. Do NOT hardcode the input size to the fully-connected layer.
- In forward(), flatten using torch.flatten(x, 1) — NEVER use x.view(-1, fixed_number).
- Utilize transfer learning if applicable; if using transfer learning, freeze all pretrained weights.
- Select the best fitting deep learning architecture for the dataset.
- Replace the final fully connected layer with the correct output dimension based on the dataset.

EXAMPLE STRUCTURE (follow this pattern):

    # Code outline...

2.  requirements.txt - List all pip packages needed (one per line). Include version constraints if important. Packages: torch, torchvision, pandas, numpy, pillow.

OUTPUT FORMAT - Use this EXACT structure: 
=== requirements.txt ===
package1
package2>=version

=== model.py === 
import package1 # … rest of the file

IMPORTANT: Use the === markers exactly as shown. No markdown code fences, no explanations.