Requirement already satisfied: torch in /opt/pytorch/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.8.0+cu129)
Requirement already satisfied: torchvision in /opt/pytorch/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.23.0+cu129)
Requirement already satisfied: pandas in /opt/pytorch/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.3.3)
Requirement already satisfied: numpy in /opt/pytorch/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.26.4)
Requirement already satisfied: pillow in /opt/pytorch/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (12.0.0)
Requirement already satisfied: filelock in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.20.0)
Requirement already satisfied: typing-extensions>=4.10.0 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)
Requirement already satisfied: setuptools in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (80.9.0)
Requirement already satisfied: sympy>=1.13.3 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)
Requirement already satisfied: networkx in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.5)
Requirement already satisfied: jinja2 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)
Requirement already satisfied: fsspec in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.9.86)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.9.79)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.9.79)
Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)
Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.9.1.4)
Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (11.4.1.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (10.3.10.19)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (11.7.5.82)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.5.10.65)
Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (0.7.1)
Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (2.27.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.9.79)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (12.9.86)
Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (1.14.1.1)
Requirement already satisfied: triton==3.4.0 in /opt/pytorch/lib/python3.12/site-packages (from torch->-r requirements.txt (line 1)) (3.4.0)
Requirement already satisfied: python-dateutil>=2.8.2 in /opt/pytorch/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /opt/pytorch/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /opt/pytorch/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)
Requirement already satisfied: six>=1.5 in /opt/pytorch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/pytorch/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /opt/pytorch/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)
================================================================================
ðŸ¤– AutoML Agent - Direct Mode
================================================================================
Path is not a valid file or does not exist: 
You are to generate TWO files:

1. model.py - A Python file that includes:
- A train() function that trains the model for multiple epochs.
- A validate() function that evaluates the model on a validation dataset and reports metrics.
- An if __name__ == '__main__' block with argparse CLI.
- Only output the code.

CRITICAL REQUIREMENTS:

1. ARGPARSE INTERFACE - Use these EXACT argument names: --train (path to training data) --val (path to validation data) --epochs (number of epochs) --batch-size (batch size) --lr (learning rate). Default the epochs, batch size, and learning rate to reasonable values based on common practices.

2. DEVICE HANDLING - MUST support both CPU and GPU: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'). Ensure that the model and data are moved to the appropriate device in both train() and validate() functions.

3. TRAINING LOOP - Proper structure:
   - The train function should accept epochs as a parameter and run the full training loop. Move model and data to device inside the function.
   - train() should be called with epochs = 1 if train is being in a loop of epochs.
   - Print the average loss per epoch (not just the last batch).
   - In the main block, call train() ONCE with all epochs, not in a loop.

4. LOGGING - Print after each epoch: Epoch X/Y: Train Loss: 0.XXXX, Train Acc: XX.XX%, Val Loss: 0.XXXX, Val Acc: XX.XX%.

5. CHECKPOINTING - Save the model when the validation metric improves: torch.save(model.state_dict(), 'best_model.pth'). Print a message when saving: 'Saved best model (Val Acc: XX.XX%)'.

6. CODE QUALITY:
   - Use clear and descriptive variable names.
   - Add docstrings to all functions.
   - Handle edge cases (e.g., empty datasets, etc.).

7. DEPENDENCIES - ONLY use these libraries:
   - torch (and torch.nn, torch.optim, torch.utils.data)
   - pandas (for CSV reading)
   - numpy (optional)
   - torchvision (ONLY for ImageFolder OR transfer learning)

8. MODEL CLASS - Use a pretrained model such as ResNet18 or MobileNetV3 UNLESS IT IS A VERY SIMPLE DATASET.
    - ONLY freeze all the pretrained layers if the image dataset is compatible with ImageNet out of the box:
        for param in self.base_model.parameters():
            param.requires_grad = False.
    - If the image dataset is not compatible with ImageNet, allow finetuning on the last X layers of the pretrained model based on what makes sense. Here is an example:
        for name, param in self.base_model.named_parameters():
            if "layer3" in name or "layer4" in name or "fc" in name:
              param.requires_grad = True.
            else:
               param.requires_grad = False.
    - Replace the classifier head with a new Linear layer of correct output size.
    - Use adaptive pooling or a dummy forward pass to compute flatten size dynamically.
- Aim for >90% accuracy.
- The model MUST compute the flatten dimension dynamically during initialization using a dummy forward pass. Do NOT hardcode the input size to the fully-connected layer. Define all layers used in forward() before calling the flatten-size computation helper.
- In forward(), flatten using torch.flatten(x, 1) â€” NEVER use x.view(-1, fixed_number).

Dataset Description: Stanford Cars dataset

Dataset schema facts: {
  "train_path": "datasets/stanford_cars_processed/train/",
  "val_path": "datasets/stanford_cars_processed/test/",
  "type": "huggingface_dataset",
  "num_rows": 4072,
  "n_classes": 196,
  "split_type": "huggingface"
}

Dataset type Specific handling: 
- Task: Classification with 196 classes
- Loss: CrossEntropyLoss
- Metrics: Accuracy



If the dataset type is huggingface:
- Use huggingface datasets to load data: 
      from datasets import load_dataset
      train_hf = load_dataset(<Use whatever is passed as --train argument and remove train at end of path if it exists>, split="train")
      test_hf = load_dataset(<Use whatever is passed as --test argument and remove test at end of path if it exists>, split="test")
- The dataset contains:
 - image feature: either a PIL image or a NumPy array representing pixel values.
 - label feature: ClassLabel 
- Labels must be loaded as int and num_classes must be computed from the label feature to set the model output size:
  num_classes = train_hf.features['label'].num_classes
- If image is a NumPy array:
     -Reshape to (H, W, C) where: 
        - If num_pixels = image.size is a perfect square â†’ grayscale, C=1
        - Otherwise â†’ C=3 for color images
     -Convert to PIL: img = transforms.ToPILImage()(image)
- The preprocess function used with .map() MUST return a dict with keys 'image' and 'label', NEVER a tuple.
- DataLoaders must use collate_fn to convert dicts to (images, labels) batches.
- Apply torchvision transforms on PIL images, then convert to tensor.
- Convert single-channel grayscale tensors to 3 channels when using pretrained models.

If the dataset type is CSV Image Data:
- CSV rows contain: label + flattened pixel values
- MAKE SURE to only divide the pixel values by 255 if they are not already in the range 0 - 1. Also make sure to load pixel values as float.
- Make sure the label is loaded as int.
- Always compute the number of unique values from the label column and use that to set the number of output classes in the model.
- Automatically infer image shape and channels from row length:
      num_pixels = len(row) - 1  # exclude label
      if (num_pixels ** 0.5).is_integer():
           height = width = int(num_pixels ** 0.5)
           channels = 1  # single-channel grayscale
      else:
           height = width = int((num_pixels // 3) ** 0.5)
           channels = 3  # multi-channel image

FOR BOTH CSV AND HUGGINGFACE DATASET:
- After reshaping into (H,W,C) or (C,H,W), the model.py MUST:
      - Convert NumPy â†’ PIL using transforms.ToPILImage()
      - Apply PIL-based torchvision transforms (Resize, ColorJitter, etc.)
      - Convert back to tensor using transforms.ToTensor()        
- transforms must run on PIL images, NOT on raw tensors.
- Correct preprocessing pipeline MUST be:
      img = img.reshape(H, W, C)
      img = transforms.ToPILImage()(img)
      img = transform_pipeline(img)  
- If using pretrained models, make sure to resize image to what fits the pretrained models (Example: 224 X 224 for resnet18).
- Normalization MUST use ImageNet statistics when using pretrained models for both training and validation sets:
      mean = [0.485, 0.456, 0.406]
      std  = [0.229, 0.224, 0.225].
- Make sure to have brightness, rotation, or horizontal flip augmentation ONLY for training set. Make sure to use valid syntax for ColorJitter and don't pass invalid parameters.
- If the image is grayscale, convert to 3-channel AFTER converting to tensor:
      if tensor.shape[0] == 1:
          tensor = tensor.repeat(3,1,1).

EXAMPLE STRUCTURE (follow this pattern):

    # Code outline...

2. requirements.txt - List all pip packages needed (one per line). Include version constraints if important. Packages: torch, torchvision, pandas, numpy, pillow.

OUTPUT FORMAT - Use this EXACT structure: 
=== requirements.txt ===
package1
package2>=version

=== model.py === 
import package1 # â€¦ rest of the file
Calling LLM to generate initial code and dependencies...
âœ“ Obtained initial model from LangGraph pipeline.
âœ“ Wrote requirements.txt
âœ“ Wrote model.py

=== Iteration 1/3 ===
Wrote model.py
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchvision.models import resnet18
from PIL import Image
import numpy as np

class ImageClassifier(nn.Module):
    def __init__(self, num_classes):
        super(ImageClassifier, self).__init__()
        self.base_model = resnet18(pretrained=True)
        
        # Freeze all layers
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # Unfreeze the last layers
        for name, param in self.base_model.named_parameters():
            if "layer4" in name or "fc" in name:
                param.requires_grad = True
        
        # Replace the classifier head
        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)
        
        # Dummy forward pass to compute flatten size
        self.flatten_size = self._get_flatten_size()

    def _get_flatten_size(self):
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, 224, 224)  # Assuming input size for ResNet
            x = self.base_model(dummy_input)
            return x.shape[1]

    def forward(self, x):
        x = self.base_model(x)
        return x

def preprocess_image(image):
    """Preprocess the image for the model."""
    transform_pipeline = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    if isinstance(image, np.ndarray):
        if image.size == 1:  # Grayscale
            image = image.reshape((1, int(np.sqrt(image.size)), int(np.sqrt(image.size))))
            image = np.repeat(image, 3, axis=0)  # Convert to 3 channels
        else:
            height = width = int((image.size // 3) ** 0.5)
            image = image.reshape((height, width, 3))
        
        image = transforms.ToPILImage()(image)
    
    return transform_pipeline(image)

def collate_fn(batch):
    """Collate function to convert dicts to (images, labels) batches."""
    images = [preprocess_image(item['image']) for item in batch]
    labels = [item['label'] for item in batch]
    return torch.stack(images), torch.tensor(labels)

def train(model, train_loader, criterion, optimizer, device, epochs):
    """Train the model for a specified number of epochs."""
    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total
        print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.2f}%')

def validate(model, val_loader, criterion, device):
    """Validate the model on the validation dataset."""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train', required=True, help='Path to training data')
    parser.add_argument('--val', required=True, help='Path to validation data')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')
    args = parser.parse_args()

    # Load datasets
    train_hf = load_dataset(args.train.rstrip('/train'), split='train')
    val_hf = load_dataset(args.val.rstrip('/test'), split='test')
    
    num_classes = train_hf.features['label'].num_classes

    # Create DataLoaders
    train_loader = DataLoader(train_hf, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)
    val_loader = DataLoader(val_hf, batch_size=args.batch_size, collate_fn=collate_fn)

    # Initialize model, criterion, and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ImageClassifier(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # Training and validation loop
    best_val_acc = 0.0
    for epoch in range(args.epochs):
        train(model, train_loader, criterion, optimizer, device, 1)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        print(f'Epoch {epoch + 1}/{args.epochs}: Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        # Checkpointing
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f'Saved best model (Val Acc: {val_acc:.2f}%)')


âš ï¸ Model training produced errors:
 /opt/pytorch/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/opt/pytorch/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/opt/pytorch/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)

Model output:
 Epoch 1/1: Train Loss: 3.8037, Train Acc: 17.99%
Epoch 1/10: Val Loss: 2.6056, Val Acc: 32.47%
Saved best model (Val Acc: 32.47%)
Epoch 1/1: Train Loss: 1.7722, Train Acc: 52.28%
Epoch 2/10: Val Loss: 1.8137, Val Acc: 50.47%
Saved best model (Val Acc: 50.47%)
Epoch 1/1: Train Loss: 0.8179, Train Acc: 77.50%
Epoch 3/10: Val Loss: 1.6596, Val Acc: 54.45%
Saved best model (Val Acc: 54.45%)
Epoch 1/1: Train Loss: 0.3591, Train Acc: 90.90%
Epoch 4/10: Val Loss: 1.4309, Val Acc: 59.91%
Saved best model (Val Acc: 59.91%)
Epoch 1/1: Train Loss: 0.1725, Train Acc: 96.29%
Epoch 5/10: Val Loss: 1.4418, Val Acc: 61.90%
Saved best model (Val Acc: 61.90%)
Epoch 1/1: Train Loss: 0.1062, Train Acc: 97.62%
Epoch 6/10: Val Loss: 1.4437, Val Acc: 63.41%
Saved best model (Val Acc: 63.41%)
Epoch 1/1: Train Loss: 0.1196, Train Acc: 97.36%
Epoch 7/10: Val Loss: 1.7154, Val Acc: 57.72%
Epoch 1/1: Train Loss: 0.1564, Train Acc: 95.96%
Epoch 8/10: Val Loss: 1.9009, Val Acc: 55.30%
Epoch 1/1: Train Loss: 0.1557, Train Acc: 95.71%
Epoch 9/10: Val Loss: 1.7754, Val Acc: 58.11%
Epoch 1/1: Train Loss: 0.1228, Train Acc: 96.83%
Epoch 10/10: Val Loss: 1.7470, Val Acc: 59.57%

Extracted metrics: {'val_loss': 1.4309, 'val_acc': 63.41}
âœ… New best model found (Val Loss: 1.4309)
Refinement iteration complete â€” model updated.

=== Iteration 2/3 ===
Wrote model.py
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchvision.models import resnet50
from PIL import Image
import numpy as np

class ImageClassifier(nn.Module):
    def __init__(self, num_classes):
        super(ImageClassifier, self).__init__()
        self.base_model = resnet50(weights='DEFAULT')  # Use ResNet50 with default weights
        
        # Freeze all layers
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # Unfreeze the last layers
        for name, param in self.base_model.named_parameters():
            if "layer4" in name or "fc" in name:
                param.requires_grad = True
        
        # Replace the classifier head
        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)
        
    def forward(self, x):
        x = self.base_model(x)
        return x

def preprocess_image(image):
    """Preprocess the image for the model."""
    transform_pipeline = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.RandomHorizontalFlip(),  # Add random horizontal flip for augmentation
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    if isinstance(image, np.ndarray):
        if image.size == 1:  # Grayscale
            image = image.reshape((1, int(np.sqrt(image.size)), int(np.sqrt(image.size))))
            image = np.repeat(image, 3, axis=0)  # Convert to 3 channels
        else:
            height = width = int((image.size // 3) ** 0.5)
            image = image.reshape((height, width, 3))
        
        image = transforms.ToPILImage()(image)
    
    return transform_pipeline(image)

def collate_fn(batch):
    """Collate function to convert dicts to (images, labels) batches."""
    images = [preprocess_image(item['image']) for item in batch]
    labels = [item['label'] for item in batch]
    return torch.stack(images), torch.tensor(labels)

def train(model, train_loader, criterion, optimizer, device, epochs):
    """Train the model for a specified number of epochs."""
    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total
        print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.2f}%')

def validate(model, val_loader, criterion, device):
    """Validate the model on the validation dataset."""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train', required=True, help='Path to training data')
    parser.add_argument('--val', required=True, help='Path to validation data')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')  # Reduced learning rate
    args = parser.parse_args()

    # Load datasets
    train_hf = load_dataset(args.train.rstrip('/train'), split='train')
    val_hf = load_dataset(args.val.rstrip('/test'), split='test')
    
    num_classes = train_hf.features['label'].num_classes

    # Create DataLoaders
    train_loader = DataLoader(train_hf, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)
    val_loader = DataLoader(val_hf, batch_size=args.batch_size, collate_fn=collate_fn)

    # Initialize model, criterion, and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ImageClassifier(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # Training and validation loop
    best_val_acc = 0.0
    for epoch in range(args.epochs):
        train(model, train_loader, criterion, optimizer, device, 1)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        print(f'Epoch {epoch + 1}/{args.epochs}: Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        # Checkpointing
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f'Saved best model (Val Acc: {val_acc:.2f}%)')


âš ï¸ Model training produced errors:
 /opt/pytorch/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

Model output:
 Epoch 1/1: Train Loss: 4.6134, Train Acc: 8.78%
Epoch 1/10: Val Loss: 3.4125, Val Acc: 25.95%
Saved best model (Val Acc: 25.95%)
Epoch 1/1: Train Loss: 2.5754, Train Acc: 43.68%
Epoch 2/10: Val Loss: 1.9824, Val Acc: 53.36%
Saved best model (Val Acc: 53.36%)
Epoch 1/1: Train Loss: 1.3686, Train Acc: 71.60%
Epoch 3/10: Val Loss: 1.3117, Val Acc: 66.62%
Saved best model (Val Acc: 66.62%)
Epoch 1/1: Train Loss: 0.7470, Train Acc: 85.09%
Epoch 4/10: Val Loss: 1.0265, Val Acc: 72.68%
Saved best model (Val Acc: 72.68%)
Epoch 1/1: Train Loss: 0.4125, Train Acc: 92.89%
Epoch 5/10: Val Loss: 0.8935, Val Acc: 76.16%
Saved best model (Val Acc: 76.16%)
Epoch 1/1: Train Loss: 0.2439, Train Acc: 96.48%
Epoch 6/10: Val Loss: 0.8475, Val Acc: 76.72%
Saved best model (Val Acc: 76.72%)
Epoch 1/1: Train Loss: 0.1536, Train Acc: 97.91%
Epoch 7/10: Val Loss: 0.7965, Val Acc: 78.27%
Saved best model (Val Acc: 78.27%)
Epoch 1/1: Train Loss: 0.0971, Train Acc: 98.93%
Epoch 8/10: Val Loss: 0.7699, Val Acc: 78.34%
Saved best model (Val Acc: 78.34%)
Epoch 1/1: Train Loss: 0.0719, Train Acc: 99.20%
Epoch 9/10: Val Loss: 0.8005, Val Acc: 77.85%
Epoch 1/1: Train Loss: 0.0517, Train Acc: 99.42%
Epoch 10/10: Val Loss: 0.7799, Val Acc: 78.61%
Saved best model (Val Acc: 78.61%)

Extracted metrics: {'val_loss': 0.7699, 'val_acc': 78.61}
âœ… New best model found (Val Loss: 0.7699)
Refinement iteration complete â€” model updated.

=== Iteration 3/3 ===
Wrote model.py
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchvision.models import resnet50
from PIL import Image
import numpy as np

class ImageClassifier(nn.Module):
    def __init__(self, num_classes):
        super(ImageClassifier, self).__init__()
        self.base_model = resnet50(weights='DEFAULT')  # Use ResNet50 with default weights
        
        # Unfreeze the last layers
        for name, param in self.base_model.named_parameters():
            if "layer4" in name or "fc" in name:
                param.requires_grad = True
        
        # Replace the classifier head
        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)
        
    def forward(self, x):
        x = self.base_model(x)
        return x

def preprocess_image(image):
    """Preprocess the image for the model."""
    transform_pipeline = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.RandomHorizontalFlip(),  # Add random horizontal flip for augmentation
        transforms.RandomRotation(10),  # Add random rotation for augmentation
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    if isinstance(image, np.ndarray):
        if image.size == 1:  # Grayscale
            image = image.reshape((1, int(np.sqrt(image.size)), int(np.sqrt(image.size))))
            image = np.repeat(image, 3, axis=0)  # Convert to 3 channels
        else:
            height = width = int((image.size // 3) ** 0.5)
            image = image.reshape((height, width, 3))
        
        image = transforms.ToPILImage()(image)
    
    return transform_pipeline(image)

def collate_fn(batch):
    """Collate function to convert dicts to (images, labels) batches."""
    images = [preprocess_image(item['image']) for item in batch]
    labels = [item['label'] for item in batch]
    return torch.stack(images), torch.tensor(labels)

def train(model, train_loader, criterion, optimizer, device, epochs):
    """Train the model for a specified number of epochs."""
    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total
        print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.2f}%')

def validate(model, val_loader, criterion, device):
    """Validate the model on the validation dataset."""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train', required=True, help='Path to training data')
    parser.add_argument('--val', required=True, help='Path to validation data')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')  # Reduced learning rate
    args = parser.parse_args()

    # Load datasets
    train_hf = load_dataset(args.train.rstrip('/train'), split='train')
    val_hf = load_dataset(args.val.rstrip('/test'), split='test')
    
    num_classes = train_hf.features['label'].num_classes

    # Create DataLoaders
    train_loader = DataLoader(train_hf, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)
    val_loader = DataLoader(val_hf, batch_size=args.batch_size, collate_fn=collate_fn)

    # Initialize model, criterion, and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ImageClassifier(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # Training and validation loop
    best_val_acc = 0.0
    for epoch in range(args.epochs):
        train(model, train_loader, criterion, optimizer, device, 1)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        print(f'Epoch {epoch + 1}/{args.epochs}: Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        # Checkpointing
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f'Saved best model (Val Acc: {val_acc:.2f}%)')


âš ï¸ Model training produced errors:
 /opt/pytorch/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

Model output:
 Epoch 1/1: Train Loss: 4.5320, Train Acc: 9.15%
Epoch 1/10: Val Loss: 3.0875, Val Acc: 29.90%
Saved best model (Val Acc: 29.90%)
Epoch 1/1: Train Loss: 2.1553, Train Acc: 51.06%
Epoch 2/10: Val Loss: 1.4060, Val Acc: 64.05%
Saved best model (Val Acc: 64.05%)
Epoch 1/1: Train Loss: 0.9602, Train Acc: 77.12%
Epoch 3/10: Val Loss: 0.8769, Val Acc: 77.10%
Saved best model (Val Acc: 77.10%)
Epoch 1/1: Train Loss: 0.5257, Train Acc: 87.51%
Epoch 4/10: Val Loss: 0.7281, Val Acc: 80.05%
Saved best model (Val Acc: 80.05%)
Epoch 1/1: Train Loss: 0.3122, Train Acc: 92.90%
Epoch 5/10: Val Loss: 0.6243, Val Acc: 82.56%
Saved best model (Val Acc: 82.56%)
Epoch 1/1: Train Loss: 0.2048, Train Acc: 95.11%
Epoch 6/10: Val Loss: 0.5877, Val Acc: 83.34%
Saved best model (Val Acc: 83.34%)
Epoch 1/1: Train Loss: 0.1474, Train Acc: 96.65%
Epoch 7/10: Val Loss: 0.5566, Val Acc: 84.08%
Saved best model (Val Acc: 84.08%)
Epoch 1/1: Train Loss: 0.1045, Train Acc: 97.70%
Epoch 8/10: Val Loss: 0.5542, Val Acc: 84.42%
Saved best model (Val Acc: 84.42%)
Epoch 1/1: Train Loss: 0.0819, Train Acc: 98.37%
Epoch 9/10: Val Loss: 0.5461, Val Acc: 84.58%
Saved best model (Val Acc: 84.58%)
Epoch 1/1: Train Loss: 0.0784, Train Acc: 98.26%
Epoch 10/10: Val Loss: 0.5483, Val Acc: 84.84%
Saved best model (Val Acc: 84.84%)

Extracted metrics: {'val_loss': 0.5461, 'val_acc': 84.84}
âœ… New best model found (Val Loss: 0.5461)
Refinement iteration complete â€” model updated.
Wrote best_model.py

    You are the LEADER agent.

    You DO NOT write code.  
    You generate the FULL AND COMPLETE prompt for the MODEL AGENT to use on the next run FOR ANY CSV IMAGE DATASET.

    Here is the current model agent prompt:
    ---------------------------------------------------------
    You are to generate TWO files:

1. model.py - A Python file that includes:
- A train() function that trains the model for multiple epochs.
- A validate() function that evaluates the model on a validation dataset and reports metrics.
- An if __name__ == '__main__' block with argparse CLI.
- Only output the code.

CRITICAL REQUIREMENTS:

1. ARGPARSE INTERFACE - Use these EXACT argument names: --train (path to training data) --val (path to validation data) --epochs (number of epochs) --batch-size (batch size) --lr (learning rate). Default the epochs, batch size, and learning rate to reasonable values based on common practices.

2. DEVICE HANDLING - MUST support both CPU and GPU: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'). Ensure that the model and data are moved to the appropriate device in both train() and validate() functions.

3. TRAINING LOOP - Proper structure:
   - The train function should accept epochs as a parameter and run the full training loop. Move model and data to device inside the function.
   - train() should be called with epochs = 1 if train is being in a loop of epochs.
   - Print the average loss per epoch (not just the last batch).
   - In the main block, call train() ONCE with all epochs, not in a loop.

4. LOGGING - Print after each epoch: Epoch X/Y: Train Loss: 0.XXXX, Train Acc: XX.XX%, Val Loss: 0.XXXX, Val Acc: XX.XX%.

5. CHECKPOINTING - Save the model when the validation metric improves: torch.save(model.state_dict(), 'best_model.pth'). Print a message when saving: 'Saved best model (Val Acc: XX.XX%)'.

6. CODE QUALITY:
   - Use clear and descriptive variable names.
   - Add docstrings to all functions.
   - Handle edge cases (e.g., empty datasets, etc.).

7. DEPENDENCIES - ONLY use these libraries:
   - torch (and torch.nn, torch.optim, torch.utils.data)
   - pandas (for CSV reading)
   - numpy (optional)
   - torchvision (ONLY for ImageFolder OR transfer learning)

8. MODEL CLASS - Use a pretrained model such as ResNet18 or MobileNetV3 UNLESS IT IS A VERY SIMPLE DATASET.
    - ONLY freeze all the pretrained layers if the image dataset is compatible with ImageNet out of the box:
        for param in self.base_model.parameters():
            param.requires_grad = False.
    - If the image dataset is not compatible with ImageNet, allow finetuning on the last X layers of the pretrained model based on what makes sense. Here is an example:
        for name, param in self.base_model.named_parameters():
            if "layer3" in name or "layer4" in name or "fc" in name:
              param.requires_grad = True.
            else:
               param.requires_grad = False.
    - Replace the classifier head with a new Linear layer of correct output size.
    - Use adaptive pooling or a dummy forward pass to compute flatten size dynamically.
- Aim for >90% accuracy.
- The model MUST compute the flatten dimension dynamically during initialization using a dummy forward pass. Do NOT hardcode the input size to the fully-connected layer. Define all layers used in forward() before calling the flatten-size computation helper.
- In forward(), flatten using torch.flatten(x, 1) â€” NEVER use x.view(-1, fixed_number).

Dataset Description: {{dataset_description}}

Dataset schema facts: {{facts_json_here}}

Dataset type Specific handling: 
{{dataset_type_specific_handling}}

{{dataset_generation_code}}

If the dataset type is huggingface:
- Use huggingface datasets to load data: 
      from datasets import load_dataset
      train_hf = load_dataset(<Use whatever is passed as --train argument and remove train at end of path if it exists>, split="train")
      test_hf = load_dataset(<Use whatever is passed as --test argument and remove test at end of path if it exists>, split="test")
- The dataset contains:
 - image feature: either a PIL image or a NumPy array representing pixel values.
 - label feature: ClassLabel 
- Labels must be loaded as int and num_classes must be computed from the label feature to set the model output size:
  num_classes = train_hf.features['label'].num_classes
- If image is a NumPy array:
     -Reshape to (H, W, C) where: 
        - If num_pixels = image.size is a perfect square â†’ grayscale, C=1
        - Otherwise â†’ C=3 for color images
     -Convert to PIL: img = transforms.ToPILImage()(image)
- The preprocess function used with .map() MUST return a dict with keys 'image' and 'label', NEVER a tuple.
- DataLoaders must use collate_fn to convert dicts to (images, labels) batches.
- Apply torchvision transforms on PIL images, then convert to tensor.
- Convert single-channel grayscale tensors to 3 channels when using pretrained models.

If the dataset type is CSV Image Data:
- CSV rows contain: label + flattened pixel values
- MAKE SURE to only divide the pixel values by 255 if they are not already in the range 0 - 1. Also make sure to load pixel values as float.
- Make sure the label is loaded as int.
- Always compute the number of unique values from the label column and use that to set the number of output classes in the model.
- Automatically infer image shape and channels from row length:
      num_pixels = len(row) - 1  # exclude label
      if (num_pixels ** 0.5).is_integer():
           height = width = int(num_pixels ** 0.5)
           channels = 1  # single-channel grayscale
      else:
           height = width = int((num_pixels // 3) ** 0.5)
           channels = 3  # multi-channel image

FOR BOTH CSV AND HUGGINGFACE DATASET:
- After reshaping into (H,W,C) or (C,H,W), the model.py MUST:
      - Convert NumPy â†’ PIL using transforms.ToPILImage()
      - Apply PIL-based torchvision transforms (Resize, ColorJitter, etc.)
      - Convert back to tensor using transforms.ToTensor()        
- transforms must run on PIL images, NOT on raw tensors.
- Correct preprocessing pipeline MUST be:
      img = img.reshape(H, W, C)
      img = transforms.ToPILImage()(img)
      img = transform_pipeline(img)  
- If using pretrained models, make sure to resize image to what fits the pretrained models (Example: 224 X 224 for resnet18).
- Normalization MUST use ImageNet statistics when using pretrained models for both training and validation sets:
      mean = [0.485, 0.456, 0.406]
      std  = [0.229, 0.224, 0.225].
- Make sure to have brightness, rotation, or horizontal flip augmentation ONLY for training set. Make sure to use valid syntax for ColorJitter and don't pass invalid parameters.
- If the image is grayscale, convert to 3-channel AFTER converting to tensor:
      if tensor.shape[0] == 1:
          tensor = tensor.repeat(3,1,1).

EXAMPLE STRUCTURE (follow this pattern):

    # Code outline...

2. requirements.txt - List all pip packages needed (one per line). Include version constraints if important. Packages: torch, torchvision, pandas, numpy, pillow.

OUTPUT FORMAT - Use this EXACT structure: 
=== requirements.txt ===
package1
package2>=version

=== model.py === 
import package1 # â€¦ rest of the file
    ---------------------------------------------------------

    Here is the best performing model:
    ---------------------------------------------------------
    import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from datasets import load_dataset
from torchvision.models import resnet50
from PIL import Image
import numpy as np

class ImageClassifier(nn.Module):
    def __init__(self, num_classes):
        super(ImageClassifier, self).__init__()
        self.base_model = resnet50(weights='DEFAULT')  # Use ResNet50 with default weights
        
        # Unfreeze the last layers
        for name, param in self.base_model.named_parameters():
            if "layer4" in name or "fc" in name:
                param.requires_grad = True
        
        # Replace the classifier head
        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)
        
    def forward(self, x):
        x = self.base_model(x)
        return x

def preprocess_image(image):
    """Preprocess the image for the model."""
    transform_pipeline = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.RandomHorizontalFlip(),  # Add random horizontal flip for augmentation
        transforms.RandomRotation(10),  # Add random rotation for augmentation
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    if isinstance(image, np.ndarray):
        if image.size == 1:  # Grayscale
            image = image.reshape((1, int(np.sqrt(image.size)), int(np.sqrt(image.size))))
            image = np.repeat(image, 3, axis=0)  # Convert to 3 channels
        else:
            height = width = int((image.size // 3) ** 0.5)
            image = image.reshape((height, width, 3))
        
        image = transforms.ToPILImage()(image)
    
    return transform_pipeline(image)

def collate_fn(batch):
    """Collate function to convert dicts to (images, labels) batches."""
    images = [preprocess_image(item['image']) for item in batch]
    labels = [item['label'] for item in batch]
    return torch.stack(images), torch.tensor(labels)

def train(model, train_loader, criterion, optimizer, device, epochs):
    """Train the model for a specified number of epochs."""
    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100 * correct / total
        print(f'Epoch {epoch + 1}/{epochs}: Train Loss: {avg_loss:.4f}, Train Acc: {accuracy:.2f}%')

def validate(model, val_loader, criterion, device):
    """Validate the model on the validation dataset."""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(val_loader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--train', required=True, help='Path to training data')
    parser.add_argument('--val', required=True, help='Path to validation data')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')  # Reduced learning rate
    args = parser.parse_args()

    # Load datasets
    train_hf = load_dataset(args.train.rstrip('/train'), split='train')
    val_hf = load_dataset(args.val.rstrip('/test'), split='test')
    
    num_classes = train_hf.features['label'].num_classes

    # Create DataLoaders
    train_loader = DataLoader(train_hf, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=True)
    val_loader = DataLoader(val_hf, batch_size=args.batch_size, collate_fn=collate_fn)

    # Initialize model, criterion, and optimizer
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ImageClassifier(num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # Training and validation loop
    best_val_acc = 0.0
    for epoch in range(args.epochs):
        train(model, train_loader, criterion, optimizer, device, 1)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        
        print(f'Epoch {epoch + 1}/{args.epochs}: Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        # Checkpointing
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pth')
            print(f'Saved best model (Val Acc: {val_acc:.2f}%)')
    ---------------------------------------------------------

    Here are the corresponding logs generated from training the model:
    ---------------------------------------------------------
    Epoch 1/1: Train Loss: 4.5320, Train Acc: 9.15%
Epoch 1/10: Val Loss: 3.0875, Val Acc: 29.90%
Saved best model (Val Acc: 29.90%)
Epoch 1/1: Train Loss: 2.1553, Train Acc: 51.06%
Epoch 2/10: Val Loss: 1.4060, Val Acc: 64.05%
Saved best model (Val Acc: 64.05%)
Epoch 1/1: Train Loss: 0.9602, Train Acc: 77.12%
Epoch 3/10: Val Loss: 0.8769, Val Acc: 77.10%
Saved best model (Val Acc: 77.10%)
Epoch 1/1: Train Loss: 0.5257, Train Acc: 87.51%
Epoch 4/10: Val Loss: 0.7281, Val Acc: 80.05%
Saved best model (Val Acc: 80.05%)
Epoch 1/1: Train Loss: 0.3122, Train Acc: 92.90%
Epoch 5/10: Val Loss: 0.6243, Val Acc: 82.56%
Saved best model (Val Acc: 82.56%)
Epoch 1/1: Train Loss: 0.2048, Train Acc: 95.11%
Epoch 6/10: Val Loss: 0.5877, Val Acc: 83.34%
Saved best model (Val Acc: 83.34%)
Epoch 1/1: Train Loss: 0.1474, Train Acc: 96.65%
Epoch 7/10: Val Loss: 0.5566, Val Acc: 84.08%
Saved best model (Val Acc: 84.08%)
Epoch 1/1: Train Loss: 0.1045, Train Acc: 97.70%
Epoch 8/10: Val Loss: 0.5542, Val Acc: 84.42%
Saved best model (Val Acc: 84.42%)
Epoch 1/1: Train Loss: 0.0819, Train Acc: 98.37%
Epoch 9/10: Val Loss: 0.5461, Val Acc: 84.58%
Saved best model (Val Acc: 84.58%)
Epoch 1/1: Train Loss: 0.0784, Train Acc: 98.26%
Epoch 10/10: Val Loss: 0.5483, Val Acc: 84.84%
Saved best model (Val Acc: 84.84%)

    ---------------------------------------------------------

    Your task:
    - Analyze the problems, failures, performance, architecture, training behavior.
    - Create a brand-new, fully rewritten Model Agent Prompt.
    - This new prompt must contain EVERYTHING the Model Agent needs to generate a future model.py.
    - Keep both performance and duration of model training in mind. 
    - DO NOT reference past prompts; produce a full standalone prompt.
    - DO NOT output code.
    - Output ONLY the prompt text, no extra delimiters.

    New MODEL AGENT prompt:
    
You are to generate TWO files:

1. model.py - A Python file that includes:
- A train() function that trains the model for multiple epochs.
- A validate() function that evaluates the model on a validation dataset and reports metrics.
- An if __name__ == '__main__' block with argparse CLI.
- Only output the code.

CRITICAL REQUIREMENTS:

1. ARGPARSE INTERFACE - Use these EXACT argument names: --train (path to training data) --val (path to validation data) --epochs (number of epochs) --batch-size (batch size) --lr (learning rate). Default the epochs, batch size, and learning rate to reasonable values based on common practices.

2. DEVICE HANDLING - MUST support both CPU and GPU: device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'). Ensure that the model and data are moved to the appropriate device in both train() and validate() functions.

3. TRAINING LOOP - Proper structure:
   - The train function should accept epochs as a parameter and run the full training loop. Move model and data to device inside the function.
   - train() should be called with epochs = 1 if train is being in a loop of epochs.
   - Print the average loss per epoch (not just the last batch).
   - In the main block, call train() ONCE with all epochs, not in a loop.

4. LOGGING - Print after each epoch: Epoch X/Y: Train Loss: 0.XXXX, Train Acc: XX.XX%, Val Loss: 0.XXXX, Val Acc: XX.XX%.

5. CHECKPOINTING - Save the model when the validation metric improves: torch.save(model.state_dict(), 'best_model.pth'). Print a message when saving: 'Saved best model (Val Acc: XX.XX%)'.

6. CODE QUALITY:
   - Use clear and descriptive variable names.
   - Add docstrings to all functions.
   - Handle edge cases (e.g., empty datasets, etc.).

7. DEPENDENCIES - ONLY use these libraries:
   - torch (and torch.nn, torch.optim, torch.utils.data)
   - pandas (for CSV reading)
   - numpy (optional)
   - torchvision (ONLY for ImageFolder OR transfer learning)

8. MODEL CLASS - Use a pretrained model such as ResNet18 or MobileNetV3 UNLESS IT IS A VERY SIMPLE DATASET.
    - ONLY freeze all the pretrained layers if the image dataset is compatible with ImageNet out of the box:
        for param in self.base_model.parameters():
            param.requires_grad = False.
    - If the image dataset is not compatible with ImageNet, allow finetuning on the last X layers of the pretrained model based on what makes sense. Here is an example:
        for name, param in self.base_model.named_parameters():
            if "layer3" in name or "layer4" in name or "fc" in name:
              param.requires_grad = True.
            else:
               param.requires_grad = False.
    - Replace the classifier head with a new Linear layer of correct output size.
    - Use adaptive pooling or a dummy forward pass to compute flatten size dynamically.
- Aim for >90% accuracy.
- The model MUST compute the flatten dimension dynamically during initialization using a dummy forward pass. Do NOT hardcode the input size to the fully-connected layer. Define all layers used in forward() before calling the flatten-size computation helper.
- In forward(), flatten using torch.flatten(x, 1) â€” NEVER use x.view(-1, fixed_number).

Dataset Description: {{dataset_description}}

Dataset schema facts: {{facts_json_here}}

Dataset type Specific handling: 
{{dataset_type_specific_handling}}

{{dataset_generation_code}}

If the dataset type is huggingface:
- Use huggingface datasets to load data: 
      from datasets import load_dataset
      train_hf = load_dataset(<Use whatever is passed as --train argument and remove train at end of path if it exists>, split="train")
      test_hf = load_dataset(<Use whatever is passed as --test argument and remove test at end of path if it exists>, split="test")
- The dataset contains:
 - image feature: either a PIL image or a NumPy array representing pixel values.
 - label feature: ClassLabel 
- Labels must be loaded as int and num_classes must be computed from the label feature to set the model output size:
  num_classes = train_hf.features['label'].num_classes
- If image is a NumPy array:
     -Reshape to (H, W, C) where: 
        - If num_pixels = image.size is a perfect square â†’ grayscale, C=1
        - Otherwise â†’ C=3 for color images
     -Convert to PIL: img = transforms.ToPILImage()(image)
- The preprocess function used with .map() MUST return a dict with keys 'image' and 'label', NEVER a tuple.
- DataLoaders must use collate_fn to convert dicts to (images, labels) batches.
- Apply torchvision transforms on PIL images, then convert to tensor.
- Convert single-channel grayscale tensors to 3 channels when using pretrained models.

If the dataset type is CSV Image Data:
- CSV rows contain: label + flattened pixel values
- MAKE SURE to only divide the pixel values by 255 if they are not already in the range 0 - 1. Also make sure to load pixel values as float.
- Make sure the label is loaded as int.
- Always compute the number of unique values from the label column and use that to set the number of output classes in the model.
- Automatically infer image shape and channels from row length:
      num_pixels = len(row) - 1  # exclude label
      if (num_pixels ** 0.5).is_integer():
           height = width = int(num_pixels ** 0.5)
           channels = 1  # single-channel grayscale
      else:
           height = width = int((num_pixels // 3) ** 0.5)
           channels = 3  # multi-channel image

FOR BOTH CSV AND HUGGINGFACE DATASET:
- After reshaping into (H,W,C) or (C,H,W), the model.py MUST:
      - Convert NumPy â†’ PIL using transforms.ToPILImage()
      - Apply PIL-based torchvision transforms (Resize, ColorJitter, etc.)
      - Convert back to tensor using transforms.ToTensor()        
- transforms must run on PIL images, NOT on raw tensors.
- Correct preprocessing pipeline MUST be:
      img = img.reshape(H, W, C)
      img = transforms.ToPILImage()(img)
      img = transform_pipeline(img)  
- If using pretrained models, make sure to resize image to what fits the pretrained models (Example: 224 X 224 for resnet18).
- Normalization MUST use ImageNet statistics when using pretrained models for both training and validation sets:
      mean = [0.485, 0.456, 0.406]
      std  = [0.229, 0.224, 0.225].
- Make sure to have brightness, rotation, or horizontal flip augmentation ONLY for training set. Make sure to use valid syntax for ColorJitter and don't pass invalid parameters.
- If the image is grayscale, convert to 3-channel AFTER converting to tensor:
      if tensor.shape[0] == 1:
          tensor = tensor.repeat(3,1,1).

EXAMPLE STRUCTURE (follow this pattern):

    # Code outline...

2. requirements.txt - List all pip packages needed (one per line). Include version constraints if important. Packages: torch, torchvision, pandas, numpy, pillow.

OUTPUT FORMAT - Use this EXACT structure: 
=== requirements.txt ===
package1
package2>=version

=== model.py === 
import package1 # â€¦ rest of the file

Best model saved as best_model.py with val_loss=0.5461
